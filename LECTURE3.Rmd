---
title: "The Virtual Ecologist"
author: "NRES 746"
date: "September 10, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

NOTE: some of this demo borrows from Hadley wickham's presentation: [Simuation](http://had.co.nz/stat480/lectures/15-simulation.pdf)

## Why simulate 'fake data'?
* Formalize your understanding of the data generating process
* Formalize your understanding of how sampling methods and uncertainty potentially affect information recovery
    * Power Analysis!
    * Sampling design!
* Test whether statistical tests do what you think they should (e.g., estimate parameters correctly)! (test for bias, precision, etc.)

We have simulated data already!

## Random numbers (from known distribution)

```{r}
runif(1,0,25)   # simulate arbitrary # of samples from dist with arbitrary params
rpois(1,3.4)
rnorm(1,22,5.4)
```

First argument: n, number of samples to generate  

Subsequent arguments: parameters of the distribution 
* Always check that the distribution is parameterised the way you expect!

#### Short exercise #1:

* Generate 50 samples from $N(10,5)$
* Generate 1000 numbers from $Poisson(50)$
* Generate 10 numbers from $Beta(0.1,0.1)$


## The model

For our purposes, the model is the process from which the samples generated. 

The model could be as simple as the random numbers you were just generating. That is we might assume that the sample data were generated from a Poisson process with mean equal to the sample mean. 

**Q**: What is the "model" for the t-test??

**A**: Well, it depends on what flavor of t-test. The simplest model could be something like this: The data from sample A are drawn from a normally distributed data population with mean equal to the sample mean. The data from sample B are drawn from a normally distributed data population with a lower mean than sample A, but with the same variance. [this is a one-sided, two sample t-test with equal variance].

In general, the models we will consider are composed of **deterministic** and **stochastic** components. For example, ordinary linear regression consists of a determistic component ($y = ax + b$) and a stochastic component (residuals are normally distributed). 

### decomposing ordinary linear regression:

First, let's look at the deterministic component:
```{r}

xvals = seq(0,100,10)  # simulated x values define parameter space

deterministic_component <- function(x,a,b){
  linear <- a + b*x   # linear functional form
  return(linear)
}

expected_vals <- deterministic_component(xvals,175,-1.5)
expected_vals

plot(xvals,exp_vals)

```

Now, let's look at the stochastic component:

```{r}

stochastic_component <- function(x,variance){
  sd <- sqrt(variance)
  stochvals <- rnorm(length(x),x,sd)
  return(stochvals)
}

sim_vals <- stochastic_component(expected_vals,variance=100)

plot(xvals,sim_vals)

plot(xvals,stochastic_component(deterministic_component(xvals,10,4),1000))

# ALTERNATIVELY:

sim_vals <- rnorm(deterministic_component(xvals,10,4),sqrt(1000))

```


------------
## Replication!! 
Wherever there is randomness, we can get different results every time (that's what it means to be random!). So... if our model results can be random, we often want to run multiple replicates. No single result by itself has any great meaning.  *The 'cloud' of replicates becomes the real result!*


One common way to do replications is to use for loops. For example:
```{r}
 #### initialize:
reps <- 50
samplesize <- length(xvals)
results <- array(0,dim=c(samplesize,reps))
for(i in 1:reps){
  exp_vals <- deterministic_component(xvals,a=10,b=4)
  sim_vals <-  stochastic_component(exp_vals,1000)
  results[,i] <- sim_vals
  if(i==1){
    plot(xvals,results[,i],ylab="simulated values")
  }else{
    points(xvals,results[,i])
  }
}

```


### e.g., generate *sampling distributions* (i.e., the distribution of test statistcs for many many independent samples)

For example, the 'brute force' t-test from the first lecture:

```{r eval=FALSE}
reps <- 1000                    # number of replicate samples to generate
null_difs <- numeric(reps)              # storage vector for the test statistic for each sample

for(i in 1:reps){
  sampleA <- sample(popData_null,size=sample.size)        # sample representing "groups" A and B under the null hypothesis
  sampleB <- sample(popData_null,size=sample.size)
  null_difs[i] <- mean(sampleA)-mean(sampleB)         # test statistic (model result)
}

hist(null_difs)           # plot out the sampling distribution
abline(v=observed_dif,col="green",lwd=3)
```


**NOTE**: the results of frequentist analyses, e.g., the t-test, are often based on a single replicate from a theoretically infinite number of possibilites. However, the interpretation of the results is implicitly based on replication ("if the null hypothesis were true, and the experiment were *replicated* lots and lots of times, results as or more extreme as the observed results could be expected from x% of replicates")

## Power analysis!!

When designing experiments or field monitoring protocols, we often ask questions like:
* What sample size do I need to be able to address my research questions?
* What is the smallest effect size I can reliably detect with my sampling design?
* What sources of sampling or measurement error should I make the greatest effort to minimize?

In such cases, probably the most straightforward way to address these questions is to simulate data under various sampling designs and error structures, and see how well you can recover the information you want!

### Power analysis, example

Imagine we are designing a monitoring program for a population of an at-risk species, and we want to have at least a 75% chance of detecting a decline of 25% or more over a 10 year period. Let's assume that we are using visual counts, and that the probability of encountering each individual visually is 2% per person-day. The most recent population estimate was 1000.   

What we know:
* A single person has a 2% chance of detecting each animal in the population in a day of surveying
* The initial abundance is 1000
* We want to be able to detect a decline as small as 25% over 10 years with at least 75% probability.


First, let's set the groundwork by making some helpful functions:

This function takes the true number in the population and returns the observed number:

```{r}
NumObserved <- function(TrueN=1000,surveyors=1,days=3){
  probPerPersonDay <- 0.02
  probPerDay <- 1-(1-probPerPersonDay)^surveyors
  probPerSurvey <- 1-(1-probPerDay)^days
  nobs <- rbinom(1,size=TrueN,prob=probPerSurvey)
}
```

This function gives us the current-year abundance using last years abundance and trend information

```{r}
ThisYearAbund <- function(LastYearAbund=1000,trend=-0.03){
  CurAbund <- LastYearAbund + trend*LastYearAbund
  return(CurAbund)
}

```

This function will simulate a single dataset!
```{r}
SimulateMonitoringData <- function(InitAbund=1000,trend=-0.03,years=25,observers=1,days=3){
  prevabund <- InitAbund
  detected <- numeric(years)
  for(y in 1:years){
    thisAbund <- ThisYearAbund(prevabund,trend)
    detected[y] = NumObserved(thisAbund,observers,days)
  }
  return(detected)
}

SimulateMonitoringData()

```




```{r}
possible.ns <- seq(from=100, to=2000, by=50)     # The sample sizes we'll be considering
powers <- rep(NA, length(possible.ns))           # Empty object to collect simulation estimates
alpha <- 0.05                                    # Standard significance level
sims <- 500                                      # Number of simulations to conduct for each N

#### Outer loop to vary the number of subjects ####
for (j in 1:length(possible.ns)){
  N <- possible.ns[j]                              # Pick the jth value for N
  
  significant.experiments <- rep(NA, sims)         # Empty object to count significant experiments
  
  #### Inner loop to conduct experiments "sims" times over for each N ####
  for (i in 1:sims){
    Y0 <-  rnorm(n=N, mean=60, sd=20)              # control potential outcome
    tau <- 5                                       # Hypothesize treatment effect
    Y1 <- Y0 + tau                                 # treatment potential outcome
    Z.sim <- rbinom(n=N, size=1, prob=.5)          # Do a random assignment
    Y.sim <- Y1*Z.sim + Y0*(1-Z.sim)               # Reveal outcomes according to assignment
    fit.sim <- lm(Y.sim ~ Z.sim)                   # Do analysis (Simple regression)
    p.value <- summary(fit.sim)$coefficients[2,4]  # Extract p-values
    significant.experiments[i] <- (p.value <= alpha) # Determine significance according to p <= 0.05
  }
  
  powers[j] <- mean(significant.experiments)       # store average success rate (power) for each N
}
plot(possible.ns, powers, ylim=c(0,1))
```










