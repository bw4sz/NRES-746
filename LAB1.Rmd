---
title: "Lab Exercise 1"
author: "NRES 746"
date: "August 23, 2016"
output: 
  html_document: 
    theme: cerulean
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction to the R Programming Language and Basic Statistics  

### INTRODUCTION
In this course we will rely heavily on the R programming language for statistical computing and graphics (believe it or not, this website is actually built in R!). The purpose of this first laboratory exercise is to develop the level of familiarity with R that is needed to succeed in this course – and ultimately, to establish a foundation for you to further develop your data analysis skills using R throughout your scientific career.   

This lab exercise will extend over two laboratory periods (Sep. 1 and Sep. 8), with a report due on Sept. 17. As with all lab reports, please email the report to the instructor in MS-Word format by midnight on the date due. You will work in groups of 3 but submit individual lab reports. One student in each group should be an “R guru”, if possible. There is no page limit on this first report, which will consist of your responses to the particular exercises given. Future laboratory exercises will be far less cookbook and far more integrative, and the reports will reflect that. 

### SET UP
Log onto computers using your assigned login and initial password (instructor will provide). Open the R software from the program menu or desktop. Use the File…Change Dir menu to change the working directory to your personal folder in the course network space, or preferably a subfolder called “Lab 1”.   

### PROCEDURE
I. Go to website [http://cran.r-project.org/](http://cran.r-project.org/). This is the source for the free, public-domain R software and where you can access R extension packages, find help, access the user community, etc. The instructor will walk you through this website and provide some discussion on the R software program.   
II. From the R manual, [“Introduction to R”](http://cran.r-project.org/doc/manuals/R-intro.html) you will implement all the steps in Appendix A, located [here](http://cran.r-project.org/doc/manuals/R-intro.html#A-sample-session). This takes you far but without much explanation; it is a way to jump into the deep end of the pool. Depending on whether you are already familiar with R, you may also find the remainder of this document useful as you work your way through the course. You may proceed at your own pace but, as you work your way through this tutorial, please ask the instructor if you are uncertain about anything.
II. The ‘beginning R user’ types expressions one at a time in the R console, and sees what happens. The more advanced R user saves blocks of often-used code in text files or uses the R script window. To gain some familiarity with using R scripts, complete the following steps:
a. Review the meaning of the Central Limit Theorem, which states that the mean of a sufficiently large number of independent random variables will be approximately normally distributed. The “sufficiently large number” has been demonstrated to be approx.. n = 30, giving rise to 30 being (often unwisely) considered the universal “magic number” for an appropriate sample size.   
b. Open the R script window from the File menu. 
c. The following code for illustrating the central limit theorem was inspired by Teetor, Paul. 2011. R Cookbook. O-Reilly Media, Inc. (p. 45), with substantial modification. Type (or paste) this into your R script window:

```{r results='hide'}
    # define the true distribution
TRUEMIN <- 10      # minimum possible value for a hypothetical random variable
TRUEMAX <- 20      #  ... and the maximum

   # define the samples to be drawn from this hypothetical distribution
N_IND_SAMPLES <- 1000   # number of random samples to draw
SAMPLESIZE <- 10       # size of each sample

lots <- 100000          # define a "practically infinite" large number

datafountain <- runif(lots,TRUEMIN,TRUEMAX) # everlasting font of pure unadulterated data

samplemean <- numeric(N_IND_SAMPLES)

for(i in 1:N_IND_SAMPLES){
  sample <- sample(datafountain,SAMPLESIZE)
  samplemean[i] <- mean(sample)
} 


hist(datafountain,freq=F,ylim=c(0,1))
hist(samplemean,freq=F,add=T,col="red")
```


d. Experiment with executing this code in the following four ways:   
(a) copy and paste from the script window into the console;   
(b) use <ctrl R> key to execute line by line from within the script window;   
(c) use <ctrl A> to select the whole code block, then <ctrl R> to execute all at once;   
(d) save the script to a text file with .R extension using File…Save from menu, and then run the script using the source() function, e.g. source("H:\\Courses\\NRES746\\ Lab1_Aug 30\\CentralLimitTheorem.R")  

e. Now modify the script to show two plots side by side. The plot on the left should be the comparison of probability density functions (for population distribution and distribution of sample means) shown in the original script (above). A new plot on the right should show a frequency histogram of the sample means, scaled closely to the values of the means so that this distribution more clearly shows. You will need to add two lines of code to the appropriate places:
par(mfrow=c(1,2)) # sets up two side by side plots as one row and two columns
hist(samp.means, col=”blue”) # draws the histogram with blue-shaded bars
f. Experiment with changing the population and sample size parameters in the script (top four lines)
g. The # sign used above allows the R programmer to insert comments adjacent to snippets of code, which facilitates readability of code (and lets the programmer remember later on what he/she was thinking when coding things a certain way!). To show the instructor that you understand all code in this short script, comment every line so as to describe its precise meaning, including all variables and functions. To accomplish this you will probably need to consult the R help documentation. A simple way to achieve this is from the command line, e.g. ?rnorm will show you full documentation for the rnorm function. The commented code for the central limit theorem demonstration, modified to show two side-by-side plots, must be included in the laboratory report. 
IV. A very useful introductory R tutorial can be found here, courtesy of NCEAS, http://www.nceas.ucsb.edu/files/scicomp/Dloads/RProgramming/BestFirstRTutorial.pdf .
Please take the time to complete this tutorial, including going through the exercises. 
V. You now should know how to construct functions in R! Write the following functions, apply them as indicated, and list them in your laboratory report:
a. Coefficient of Variation. Apply this to the “height” vector in the “trees” dataset that installs with R as sample data.  
b. A function for drawing a regression line through a scatter plot. [hint: use the abline and plot functions]. Apply this function to the “height” and “volume” vectors in the “trees” dataset, and then to the “eruptions” and “waiting” vectors in the “faithful” dataset. 
c. Now add a scatter plot smoother to the above function, making the smoother span (degree of smoothing) a user-defined option. [Hint: use the lowess function].
d. Now you should have the tools you need to create a function from the central limit theorem demonstration code you have previously worked with. Allow the user to vary the MU, SD, N and n parameters, but set each with default arguments. Test the function out for different parameter combinations. 
VI. Multiple Regression Analysis, Exercise One: Air Quality Data
a. Type library(help="datasets") for a list of sample datasets that come with the core R package (some of these you have already encountered).
b. Examine the “airquality” dataset (use the “head” and “summary” functions). Note that there are missing values where ozone concentration data and solar radiation data were not collected.
c. We could ignore the missing values and conduct our regression analysis, since the default response of the lm (“linear model”) function is to omit cases with missing values in any of the specified parameters. However, to avoid problems later, we will omit them by constructing a new, “cleaned” dataset as follows:
air.cleaned <- na.omit(airquality) # you can call what’s created on the left-hand side anything you want!
Then use the “attach” function to attach to the new dataset so that you can reference its variables without referring to the dataset each time (e.g. “Ozone” instead of “air.cleaned$Ozone”)
d. Conduct a multiple linear regression of ozone concentration as a function of solar radiation, wind and temperature. Use the “lm” function to conduct an ordinary least squares (OLS) regression analysis. Explore the R help or ask the instructor if you cannot quickly figure out how to do this. 
e. Explore the regression outputs using the “summary” function, and explore regression diagnostics using, e.g. (depending on what you named the regression model object):  
plot.lm(ozone1.lm, which=1)
plot.lm(ozone1.lm, which=2)
plot.lm(ozone1.lm, which=3)
plot.lm(ozone1.lm, which=4)
hist(residuals(ozone1.lm), breaks=10)
plot(predict(ozone1.lm) ~ Ozone); abline(0,1)
If no one in your group knows why you are doing any of this or what it all means, ask the instructor! That’s why he’s hanging around the lab…
f. Consider the possibility that there may be an important interaction effect between solar radiation and temperature on influencing ozone concentrations. Explore that with a simple scatter plot where symbol size is scaled to ozone concentration:
symbols(Temp, Solar.R, Ozone/100, ylab="Solar Radiation", xlab="Temperature", main="Interaction Plot", inches=FALSE)
g. Now fit a second model that includes the interaction between solar radiation and temperature. Use the following formula to fit the interaction, “Ozone ~ Wind + Solar.R * Temp”
h. Explore regression outputs for the second model in the same way as you did for the first model without the interaction term.
i. Conduct an “Extra Sum of Squares F Test” to formally test whether the richer model (including the interaction term) fits the data significantly better than the reduced model (with fewer parameters) that lacks the interaction term. Note that the R2 value is inadequate for this purpose because R2 will always increase with additional parameters. Use the following syntax, 
anova(ozone1.lm, ozone2.lm, test="F")
j. Briefly answer the following questions in the laboratory report:
1.	On average, and for constant conditions of solar radiation and wind, by how much does ozone concentration increase (or decrease) for each 10-unit increase in temperature?
2.	How is the t-value calculated in the regression table? What does it represent?
3.	What is the nature of the hypothesis that the p-values for the individual coefficients in the regression table) are designed to test?
4.	Which regression model provides the more parsimonious fit (i.e. which is the best model), the model with or without the interaction term? Explain your reasoning.
5.	Interpret the diagnostic plots. Does your regression models appear to be robust, or is further analysis and refinement needed? 


VII. Multiple Regression Analysis, Exercise Two: Tree Data from a forest in the western Oregon Cascades
We will fit a multiple linear regression model that predicts forest tree biomass as a function of environmental variables (including a mix of continuous and categorical predictors) and stand age. We will assess regression diagnostics, interpret the model and report the relevant effect sizes.
Obtain the TreeData.xls file from the /public/ folder. View this file in Excel. This describes a subset of forest inventory data from the Douglas-fir forests of western Oregon (n = 90, 0.1-ha sites). 
Arranged in columns from left to right, variables are:
Site: site identifier
Biomass: tree biomass (for all species) in Mg/ha, the response variable for Part 1 of the lab. 
ABPR: the response variable of interest for the logistic regression part of the lab (next week), representing the presence/absence of Abies procera (noble fir) on a given site (coded 1 for presence). 
StandAge: Maximum tree age in the 0.1-ha plot. This variable will be used as a proxy for successional stage. We assume that stand-replacing fires are the dominant form of disturbance and that stand age is a reasonable proxy variable for time since the last fire. 
X, Y: geographic coordinates – UTM easting and northing, respectively
Elev: elevation (m)
Northeastness: slope aspect that has been linearized using a cosine transformation so that the aspect of 45 degrees has value 1 and aspect of 225 degrees has value -1. 
 
In this study area, this variable is expected to reflect a moisture gradient from moister (NE) to drier (SW) aspects.
Slope: slope steepness (degrees)
SlopePos: slope position, a categorical variable (i.e. factor) with three values: Valley, Slope and Ridge.

Save this file to a comma-delimited (.csv) file. This is a common file format for importing data into R. Import the data into R as a data frame (R’s data format for a flexible collection of variables that may be of various data types), using the following command:

NobleFir.df <- read.csv("TreeData.csv") 

Inspect the resulting data object. Summarize it using the “summary” and “plot” functions. 
Obtain a correlation matrix for biomass and the four numeric predictor variables using the “cor” function and by subscripting column locations on the data frame (ask instructor for explanation of syntax if needed):
cor(NobleFir.df[,c(2,4,7:9)])
Are any of the predictor variables highly correlated? Later in the course we will address problems of model selection, model fitting and multicollinearity. 
Calculate Box Plots for the continuous predictor variables (excluding x and y coordinates) according to sites with or without noble fir. Use the “boxplot” function. What clear relationships, if any, emerge for how sites with and without noble fir differ with regard to their environmental setting?
Use multiple linear regression to model A. procera biomass as a function of predictor variables (excluding the spatial coordinates), using the same approach for regression fitting and diagnostics as previously. 
Rerun the regression to obtain standardized regression coefficients, allowing direct comparison of effect sizes for the continuous predictor variables (since all variables are then transformed to standard deviate units, i.e. mean centered on zero with standard deviation of one). The “scale” function provides an easy way to implement this. 
Biomass_std.lm <- lm(scale(Biomass) ~ scale(elev) + scale(Northeastness) + scale(Slope) + SlopePos + scale(StandAge))
Visually assess whether regression errors (residuals) are spatially autocorrelated using the “symbols” function:
symbols(x,y,circles=abs(residuals(MyModelNameHere)), inches=0.3, ylab="Northing", xlab="Easting", main="Errors from Biomass Regression Model")
Later in the course, we will explore more sophisticated ways to quantify and model correlated error terms in regression models. 
Answer the following questions in the laboratory report:
Can forest biomass be reliably predicted by topographic variables and stand age? Does the model provide a strong and unbiased fit to the data? Is the regression statistically significant, and what does this mean? Is there spatial variation in model goodness of fit? Which of the environmental influences are most important? Do these effects make ecological sense? Interpret your regression model according to the relevant estimates of effect sizes and their variability. Interpret the effects of all predictor variables, continuous and categorical. Express their influences on forest biomass in meaningful ways, such as the effect of a 100-m increase in elevation.

