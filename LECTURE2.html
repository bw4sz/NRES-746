<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="NRES 746" />

<meta name="date" content="2016-08-31" />

<title>Working with Probabilities</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="site_libs/highlight/default.css"
      type="text/css" />
<script src="site_libs/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">NRES 746, Fall 2016</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="schedule.html">Course Schedule</a>
</li>
<li>
  <a href="labschedule.html">Lab Schedule</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lectures
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="INTRO.html">Introduction to NRES 746</a>
    </li>
    <li>
      <a href="LECTURE1.html">Why focus on algorithms?</a>
    </li>
    <li>
      <a href="LECTURE2.html">Working with probabilities</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Lab exercises
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="LAB1.html">Lab 1: Intro to R</a>
    </li>
  </ul>
</li>
<li>
  <a href="Syllabus.pdf">Syllabus</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Data sets
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="TreeData.csv">Tree Data</a>
    </li>
  </ul>
</li>
<li>
  <a href="Links.html">Links</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Working with Probabilities</h1>
<h4 class="author"><em>NRES 746</em></h4>
<h4 class="date"><em>August 31, 2016</em></h4>

</div>


<p>Note: some materials borrowed from White and Morgan’s “STATISTICAL MODELS IN ECOLOGY USING R” course</p>
<p>In the last class, we reviewed basic programming in R, using basic constructs like the programming loop (e.g., “for”, “while”) and random sampling (e.g., using the ‘sample’ function in R) to build our own analyses from first principles. We began the course this way because the ability to understand, use, and build algorithms is absolutely fundamental to modern data analysis.</p>
<p>Also fundamental to modern data analysis is an ability to work with probabilities. Again, many of you will find this a very basic review, but I really want to ensure that all of us are working with solid foundations before we venture into more advanced topics.</p>
<p>The central points: * Most traditional statistics utilize tricks &amp; assumptions to ensure the data follow a particular distribution (usually normal) * With more computational power, we are much less limited and can model alternative distributions more easily</p>
<div id="basic-probability-rules" class="section level2">
<h2>Basic probability rules</h2>
<div id="classic-urn-example" class="section level3">
<h3>Classic Urn Example</h3>
<p>Consider an Urn filled with blue, red, and green spheres. To make the example more concrete, assume the following: * red: 104 * blue: 55 * green: 30</p>
<pre class="r"><code>n_red &lt;- 104
n_blue &lt;- 55
n_green &lt;- 30

allSpheres &lt;- c(n_red,n_blue,n_green)
names(allSpheres) &lt;- c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;)</code></pre>
<p>What is the probability of drawing a blue sphere?</p>
<pre class="r"><code>P_blue &lt;- allSpheres[&quot;blue&quot;]/sum(allSpheres)
P_blue</code></pre>
<pre><code>##      blue 
## 0.2910053</code></pre>
<p>Let’s generate a vector of probabilities for drawing each type of sphere…</p>
<pre class="r"><code>Probs &lt;- allSpheres/sum(allSpheres)
Probs</code></pre>
<pre><code>##       red      blue     green 
## 0.5502646 0.2910053 0.1587302</code></pre>
<p>What is the probability of drawing a blue <strong>OR</strong> a red sphere?</p>
<pre class="r"><code>as.numeric( Probs[&quot;blue&quot;] + Probs[&quot;red&quot;] )</code></pre>
<pre><code>## [1] 0.8412698</code></pre>
<p>What is the probability of drawing a blue <strong>OR</strong> a red sphere <strong>OR</strong> a green sphere?</p>
<pre class="r"><code>as.numeric( Probs[&quot;blue&quot;] + Probs[&quot;red&quot;] + Probs[&quot;green&quot;] )</code></pre>
<pre><code>## [1] 1</code></pre>
<p>What would it mean if this didn’t sum to 1?</p>
<p>What is the probability of drawing a blue <strong>AND THEN</strong> a red sphere?</p>
<pre class="r"><code>as.numeric( Probs[&quot;blue&quot;] * Probs[&quot;red&quot;] )</code></pre>
<pre><code>## [1] 0.1601299</code></pre>
<p>What is the probability of drawing a blue and a red in two consecutive draws?</p>
<pre class="r"><code>as.numeric( (Probs[&quot;blue&quot;] * Probs[&quot;red&quot;]) + (Probs[&quot;red&quot;] * Probs[&quot;blue&quot;]) )</code></pre>
<pre><code>## [1] 0.3202598</code></pre>
</div>
<div id="less-classic-urn-example" class="section level3">
<h3>Less-classic urn example</h3>
<p>Now consider an urn filled with blue &amp; red objects of two types: spheres &amp; cubes. To make the example more concrete, assume the following: * red sphere: 39 * blue sphere: 76 * red cube: 101 * blue cube: 25</p>
<pre class="r"><code>n_red_sphere &lt;- 39
n_blue_sphere &lt;- 76
n_red_cube &lt;- 101
n_blue_cube &lt;- 25

allSpheres &lt;- c(n_red_sphere,n_blue_sphere)
allCubes &lt;- c(n_red_cube,n_blue_cube)
allTypes &lt;- c(allSpheres,allCubes)
allTypes &lt;- matrix(allTypes,nrow=2,ncol=2,byrow=T)
rownames(allTypes) &lt;- c(&quot;sphere&quot;,&quot;cube&quot;)
colnames(allTypes) &lt;- c(&quot;red&quot;,&quot;blue&quot;)
allTypes</code></pre>
<pre><code>##        red blue
## sphere  39   76
## cube   101   25</code></pre>
<pre class="r"><code>Prob_Shape &lt;- apply(allTypes,1,sum)/sum(allTypes)
Prob_Shape</code></pre>
<pre><code>##    sphere      cube 
## 0.4771784 0.5228216</code></pre>
<pre class="r"><code>Prob_Color &lt;- apply(allTypes,2,sum)/sum(allTypes)
Prob_Color</code></pre>
<pre><code>##       red      blue 
## 0.5809129 0.4190871</code></pre>
<p>What is the <em>marginal probability</em> of drawing a red object (and why do we call it a “marginal” probability?)</p>
<pre class="r"><code>Prob_Color[&quot;red&quot;]</code></pre>
<pre><code>##       red 
## 0.5809129</code></pre>
<p>What is the <em>joint probability</em> of drawing an object that is both blue AND a cube?</p>
<pre class="r"><code>as.numeric( Prob_Color[&quot;blue&quot;] * Prob_Shape[&quot;cube&quot;])  </code></pre>
<pre><code>## [1] 0.2191078</code></pre>
<p>Is this correct? If not, why?</p>
<p>Under what circumstances would this be correct?</p>
<p>What is the correct answer?</p>
<pre class="r"><code>(allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;]   </code></pre>
<pre><code>## [1] 0.1037344</code></pre>
<p>What is the probability of drawing an object that is blue <strong>OR</strong> a cube? <span class="math inline">\(Prob(blue)\bigcup Prob(cube)\)</span></p>
<pre class="r"><code>as.numeric( Prob_Color[&quot;blue&quot;] + Prob_Shape[&quot;cube&quot;])  </code></pre>
<pre><code>## [1] 0.9419087</code></pre>
<p>Is this correct? If not, why?</p>
<p>What is the correct answer?</p>
<pre><code>## [1] 0.7228009</code></pre>
<pre><code>## [1] 0.7228009</code></pre>
<p>What is the <strong>conditional probability</strong> of getting a blue object, given that it is a cube? <span class="math inline">\(Prob(blue|cube)\)</span></p>
<p>This can be expressed as: <span class="math inline">\(Prob(blue|cube) = Prob(blue,cube) / Prob(cube)\)</span></p>
<pre class="r"><code>(allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;] / Prob_Shape[&quot;cube&quot;]</code></pre>
<pre><code>##      cube 
## 0.1984127</code></pre>
<p>Can we now express the joint probability of drawing a blue cube in terms of conditional probabilities?</p>
<p>Prob(blue) Prob(cube) = Prob(blue) * Prob(cube|blue)</p>
<p>Does this now give us the correct answer?</p>
<pre class="r"><code>as.numeric( Prob_Color[&quot;blue&quot;] * (allTypes/sum(allTypes))[&quot;cube&quot;,&quot;blue&quot;] / Prob_Color[&quot;blue&quot;] )</code></pre>
<pre><code>## [1] 0.1037344</code></pre>
</div>
</div>
<div id="frequentism-vs.bayesianism" class="section level2">
<h2>Frequentism vs. Bayesianism</h2>
<p>Two types of conditional probabilities: P(data|hypothesis) is a likelihood – the probability of getting our data, given a particular hypothesis. Frequentists use this approach. Given a null hypothesis, what is the probability of getting these data (or data more extreme) - this is the definition of a pvalue. The likelihood is understood in terms of long-term frequencies – if we collected similar data many, many times, what fraction of the time would we get the outcome we observed (if the null hypothesis is true)? Developed by Fisher &amp; contemporaries. P(hypothesis|data) is called a posterior probability. It is the probability of getting our hypothesis, given the data we collected. This is how we usually interpret likelihoods and p-values, but the two things are not the same! Bayesians use posteriors, and they feel superior because P(hypothesis|data) is what we all really want to know, anyway. However, Frequentists make fun of Bayesians for doing this, because there are some potential problems in the Bayesian approach. Do not underestimate the vitriol between Frequentists and Bayesians. Also note that the seriousness with which people commit to one side or the other often corresponds to a propensity to show up at a conference wearing shorts of an improperly small size (this is a good example of a conditional probability). As a shorthand, we will often write likelihoods as P(X|θ) or L(X| θ) or L(θ) (you have to use LaTex to make the funny L symbol that Bolker uses) and posteriors as P(θ |X). To use posteriors, we use Bayes’ Law: P(θ|X) = P(X|θ) P(θ)/ P(X) That is: posterior = likelihood × prior / denominator Bayes’ Law is not constroversial. What is controversial is that Bayesians plug in the prior probability of the hypothesis, P(θ). This is often interpreted as the a priori expectation of a particular state of nature, and can be somewhat subjective. It’s this subjectivity that is controversial. The other tricky thing about Bayes’ Law is the denominator, which requires summing up the probability of all possible ways of getting the data. This is easy when there is a fairly discrete set of hypotheses (as in the Monty Hall example) but gets harder in other cases. Markov chain – Monte Carlo techniques were essentially invented to calculate the denominator in Bayes’ Law. * Note that I think I switched the prior and denominator when I wrote up Bayes Law on the boad 4. Example of using probability and Bayes’ Law: Let’s Make a Deal The setup: 3 doors, 1 hides a prize and the other hide goats or donkeys or yaks or something that would be desirable in Tashkent but not in Boca Raton. You pick door A. Before you see what’s behind door A, the host, Monty Hall, opens door C to reveal a goat. Now you can stay with A or switch to door B. Should you switch? Note that no matter what door you choose at first, Monty will always open one of the other doors, and will never open the door with the prize (he knows where the prize is). To do this we want to know P(X|I) and P(B|I), where X = the door is behind door X, and I = the information that you picked door A and Monty showed you the goat behind door C. First, calculate the priors: P(A) = P(B) = P(C) = 1/3 Now, the likelihoods. P(I|A) = ½, because if the prize is behind door A, Monty can show you either door B or door C. So the likelihood of the data (he shows you door C) given the hypothesis (the prize is behind door A) is only ½. P(I|A) = 1, because if the prize is behind door B, Monty can only show you what’s behind door C. It is the only option available to him. In other words, the likelihood of the data (he shows you door C) given the hypothesis (the prize is behind door B) is 1. Now we need the denominator, P(I). The easiest way to get this unconditional probability is to rearrange the equation for conditional probability: P(X,I) = P(I|X)P(X) so P(I) = all possible ways of getting P(I): P(I) = P(I|A)P(A) + P(I|B)P(B) + P(I|C)P(C) (either A is true and you observe I, or B is true and you observe I, or C is true and you observe I). P(I) = ½ × 1/3 + 1 × 1/3 + 0 × 1/3 = ½ So put it all together: P(A|I) = (1/2) ×(1/3)/(1/2) = 1/3 P(B|I) = (1) ×(1/3)/(1/2) = 2/3 So it is always better to switch. I have posted some R code on the webpage that simulates this game to confirm this result. Notice that in this case, the ratio of the two likelihoods (1 to 1/2) was the same as the ratio of the two posteriors (2/3 to 1/3). This is true only because the priors were all the same (this would be called “flat” or “uninformative” priors). However, if we had more prior information – say, there was a strong smell of goat coming from door A, then the priors would be different ( P(A) &gt; P(B) ). The likelihoods would remain exactly the same, but the posterior for A would be greater than 1/3. That’s the value of the Bayesian approach – it can combine all available information. 5. Features of probability distributions Discrete vs. continuous In discrete distributions, each outcome has a specific probability ( like the probability of flipping a coin 10 times and getting 4 heads). In continuous distributions, the height of the curve corresponds to probability density, f(x), not probability P(x). This is because the probability of getting exactly one value in a continuous distribution is effectively zero. This arises from the problem of precision. The sum of the probability distribution must be 1 (there is only 100% of probability to go around). So the sum of all P(x) must be 1. But in a continuous distribution, there are an infinite number of values of x (think about it – between x = 2.01 and x = 2.02 are the numbers 2.001, 2.002, 2.003, …, and between those are 2.0001, 2.0002, 2.0003, … The number line is infinitely divisible). So any individual probability is always divided by infinity, which makes it zero. The long and short of this is that we have to talk about probability density, unless we want to specify a particular range of values – we can’t calculate P(x = 5), but we can calculate P(4 &lt; x &lt; 6) or P(x &gt; 5). For this reason, when dealing with continuous distributions, the likelihood P(θ|H) will have to be a probability density f(θ|H). So likelihoods are not always probabilities. Some other distributional lingo: Moments – descriptions of the shaped &amp; central tendency of the distribution Parameters – the values in the probability distribution function that define the shape of the pdf. Parametric statistics require assuming certain things about distributions &amp; parameters, while nonparametric stats do not require these assumptions. 6. Some probability distributions Most of the distributions we use were originally formulated in industrial applications and then rejiggered for ecology. That’s why most of them deal with waiting times, failures, etc. Bolker explains the main ones. Pay particular attention to the type of process described by each distribution. The key to using these correctly is to figure out which statistical process best matches the ecological process you’re studying, then use that distribution. e.g., Am I observing independent, random events occurring in a fixed window of time or space (like sampling barnacles in quadrats on an intertidal bench)? Then the distribution of their occurrence probably follows a Poisson distribution.</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
